{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"sentiment_analysis_2.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1KwKe3F4g0xlJ09mV56M_TmRpLW9MxA7S","authorship_tag":"ABX9TyMSAcSQtW5hRPEyEqswullZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"yw5Zz2NYEE-k"},"source":["# 댓글 데이터 감성분석\n","- Aurora3 기반 https://github.com/gyubok-lee/Aurora3\n","- Google colaboratory 이용시 drive mount & cd 설정 필요\n"]},{"cell_type":"code","metadata":{"id":"qJdmaAk1SqKz"},"source":["\"\"\"\n","cd /content/drive/MyDrive/경로설정\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fFehu9X3RuLL"},"source":["# konply 설치\n","!pip install konlpy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"psetTOI5RuBj"},"source":["# module & package 가져오기\n","\n","import pandas as pd\n","import re\n","import konlpy\n","import matplotlib.pyplot as plt\n","from matplotlib import font_manager, rc\n","import numpy as np\n","from tqdm import tqdm\n","import urllib.request\n","import glob\n","\n","from sklearn.preprocessing import MinMaxScaler\n","scaler = MinMaxScaler(feature_range=(-2, 2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OQypLXv63Ef3"},"source":["-  작업순서\n","\n","\n","- input data 파일 형식 바꿔주기\n","- test, train data 불러오기\n","- 데이터 전처리 작업\n","- 단어별로 감성점수 측정, 추정\n","- 댓글 문장 별 감성분석\n","- score 측점 결과 저장"]},{"cell_type":"markdown","metadata":{"id":"OX65f35TsU-K"},"source":["# 데이터 전처리 및 파일 생성"]},{"cell_type":"code","metadata":{"id":"ATMy4ViupY0N"},"source":["# 본인이 설정한 디렉토리에 있는 파일 리스트화 시키기\n","\n","file_list = glob.glob('.././*')\n","file_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZscFRaRapqAu"},"source":["# 개수 확인\n","\n","len(file_list) # len() = 96"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gcZ4nEjsptyH"},"source":["# 파일명만 떼어서 리스트화\n","\n","f_names = []\n","for i in range(96):\n","  f_name = re.sub(\"../data_xlsx/\",\"\",file_list[i])\n","  f_names.append(f_name)\n","f_names"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oD6Nd-MHp75B"},"source":["# 변환될 파일명 리스트화\n","\n","f_names_new = []\n","for i in range(96):\n","  f_name = re.sub(\"../data_xlsx/\",\"\",file_list[i])\n","  f_name = re.sub(\".xlsx\",\".txt\",f_name)\n","  f_names_new.append(f_name)\n","f_names_new"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z0jvANNXqRUW"},"source":["# 리스트화 잘 되었는지 확인\n","\n","print(f_names[0])\n","type(f_names[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DJawrSr0qWJo"},"source":["# 원하는 디렉토리에 파일 한 개씩 저장\n","\n","for i in range(96):\n","  df = pd.read_excel(f'data_xlsx/{f_names[i]}')\n","  print(f_names[i])\n","  df.to_csv(f'data_txt/{f_names_new[i]}',index=True, sep='\\t')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0aItblLlq2ak"},"source":["# dataframe화 잘 되는지 체크\n","\n","df = pd.read_table(f'data_txt/{f_names_new[0]}')\n","df = df.iloc[:,:] \n","# 기존 row name을 data에 맞게 변경\n","df = df[['videoId','comment']]\n","df = df.rename(columns={\"videoId\": \"id\", \"comment\": \"sentence\"})\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O3ym4dTHrdY7"},"source":["# class, functions 정의 및 score 도출 작업"]},{"cell_type":"code","metadata":{"id":"m5vU22qErjTR"},"source":["# 데이터 전처리 및 토큰화 작업\n","\n","okt = konlpy.tag.Okt()\n","\n","def text_preprocess(x):\n","    text=[]\n","    \"\"\"\n","    <br>, <a href~ > 등의 태그 제거 작업 추가 필요\n","    \"\"\"\n","    x = re.sub(\"<.+?>\",\"\",x)\n","    a = re.sub('[^가-힣0-9a-zA-Z\\\\s]', '',x)\n","    for j in a.split():\n","        text.append(j)\n","    return ' '.join(text)\n","\n","def tokenize(x):\n","    text = []\n","    tokens = okt.pos(x)\n","    for token in tokens :\n","        if token[1] == 'Adjective' or token[1]=='Adverb' or token[1] == 'Determiner' or token[1] == 'Noun' or token[1] == 'Verb' or 'Unknown':\n","            text.append(token[0])\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O_5Pd7DIScCT"},"source":["# rule-based 감성분석 점수 사전 업데이트\n","\n","sent_dic = pd.read_csv('SentiWord_Dict.txt',sep = '\\t',header=None)\n","sent_dic.iloc[14850,0]='갈등'\n","\n","pos_dic = sent_dic[sent_dic[1]>0]\n","neg_dic = sent_dic[sent_dic[1]<0]\n","neu_dic = sent_dic[sent_dic[1]==0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VuC83E8qEE_r"},"source":["# class 선언으로 한번에 작업 처리\n","\n","class Aurora3:\n","    \n","    def __init__(self, df,sent_dic):\n","        self.df = df\n","        self.okt = konlpy.tag.Okt()\n","        self.sent_dic = sent_dic\n","        \n","    def get_df(self):# 최종 결과 반환\n","        #print(\"문장 토큰화 중입니다...\")\n","        #self.tokenizer_run()\n","        \n","        print(\"감성사전 업데이트 중입니다...\")\n","        self.expand_sent_dic()\n","        \n","        print(\"문장 감성분석 중입니다....\")\n","        self.sent_analyze()\n","        return self.df\n","    \"\"\"    \n","    def tokenizer_run(self): # 텍스트 전처리 & 토큰화\n","        tqdm.pandas()\n","        \n","        def text_preprocess(x): \n","            text=[]\n","            \n","            x1 = re.sub(\"<.+?>\",\"\",x)\n","            a = re.sub('[^가-힣0-9a-zA-Z\\\\s]', '',x1)\n","            for j in a.split():\n","                text.append(j)\n","            return ' '.join(text)\n","\n","        def tokenize(x):\n","            text = []\n","            tokens = self.okt.pos(x)\n","            for token in tokens :\n","                if token[1] == 'Adjective' or token[1]=='Adverb' or token[1] == 'Determiner' or token[1] == 'Noun' or token[1] == 'Verb' or 'Unknown':\n","                    text.append(token[0])\n","            return text\n","\n","        self.df['comment_cut'] = self.df['sentence'].apply(lambda x : text_preprocess(x))\n","        self.df['comment_cut'] = self.df['comment_cut'].progress_apply(lambda x: tokenize(x))\n","    \"\"\"\n","    def expand_sent_dic(self):\n","        sent_dic = self.sent_dic\n","        \n","        def make_sent_dict(x) :\n","            pos=[]\n","            neg=[]\n","            tmp={}\n","\n","            for sentence in tqdm(x):\n","                for word in sentence :\n","                    target = sent_dic[sent_dic[0]==word]\n","                    if len(target)==1: # 기존에 있는 단어라면 그대로 사용\n","                        score = float(target[1])\n","                        if score > 0:\n","                            pos.append(word)\n","                        elif score < 0:\n","                            neg.append(word)                \n","                    tmp[word] = {'W':0,'WP':0,'WN':0} # 감성사전 구성\n","            pos = list(set(pos))\n","            neg = list(set(neg))\n","\n","            for sentence in tqdm(x):\n","                for word in sentence :\n","                    tmp[word]['W'] += 1 # 빈도 수\n","                    for po in pos :\n","                        if po in sentence:\n","                            tmp[word]['WP'] += 1 # 긍정단어과 같은 문장 내 단어일 때\n","                            break\n","                    for ne in neg:\n","                        if ne in sentence:\n","                            tmp[word]['WN'] += 1 # 부정단어와 같은 문장내 단어일 때\n","                            break\n","            return pos, neg, pd.DataFrame(tmp)\n","        \n","        def make_score_dict(d,p,n):\n","            N=sum(d.iloc[0,::])\n","            pos_cnt=sum(d.loc[::,p].iloc[0,::])\n","            neg_cnt=sum(d.loc[::,n].iloc[0,::])\n","\n","            trans =d.T\n","            trans['neg_cnt']=neg_cnt\n","            trans['pos_cnt']=pos_cnt\n","            trans['N']=N\n","\n","            trans['MI_P']=np.log2(trans['WP']*trans['N']/trans['W']*trans['pos_cnt'])\n","            trans['MI_N']=np.log2(trans['WN']*trans['N']/trans['W']*trans['neg_cnt'])\n","            trans['SO_MI']=trans['MI_P'] - trans['MI_N']\n","\n","            trans = trans.replace([np.inf, -np.inf], np.nan).dropna(axis=0)\n","            trans = trans.sort_values(by=['SO_MI'],ascending=False)\n","            return trans\n","        \n","        def update_dict(d):\n","            add_Dic = {0:[],1:[]}\n","            for i in d.T.items():\n","                if i[0] not in list(sent_dic[0]):\n","                    if len(i[0]) > 1:\n","                        add_Dic[0].append(i[0])\n","                        add_Dic[1].append(i[1]['SO_MI'])\n","\n","            add_Dic=pd.DataFrame(add_Dic)\n","            Sentiment=pd.merge(sent_dic,add_Dic,'outer')\n","            return Sentiment\n","        \n","        self.pos, self.neg, self.new_dict = make_sent_dict(self.df['comment_cut'].values)\n","        \n","        self.t_dict = make_score_dict(self.new_dict,self.pos,self.neg)\n","        self.t_dict['SO_MI'] = scaler.fit_transform(self.t_dict['SO_MI'].values.reshape(-1,1))\n","       \n","        self.add_dict =update_dict(self.t_dict)\n","    \n","    def sent_analyze(self): # 데이터 감성분석\n","        tqdm.pandas()\n","        \n","        def get_cnt(x):\n","            cnt = 0\n","            for word in list(set(x)):\n","                target = self.add_dict[self.add_dict[0]==word]\n","                if len(target)==1:\n","                    cnt += float(target[1])\n","            return cnt\n","\n","        def get_ratio(x): # log로 score 정규화\n","            score = x['score']\n","            length = np.log10(len(x['comment_cut']))+1\n","            try:\n","                ratio= round(score/length,2)\n","            except:\n","                ratio = 0\n","            return ratio\n","        \n","        tqdm.pandas()\n","        self.df['score']= self.df['comment_cut'].progress_apply(lambda x : get_cnt(x))\n","        self.df['ratio'] = self.df.apply(lambda x: get_ratio(x), axis = 1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GZ4wm_JrsF_7"},"source":["# 점수 도출 및 파일 저장"]},{"cell_type":"code","metadata":{"id":"07sPa21fBh-h"},"source":["for i in range(96):\n","  # 반복 횟수 설정은 원하는 길이대로\n","  df = pd.read_table(f'data_txt/{f_names_new[i]}')\n","  #영화별로 총 댓글 수 다름 -> 자동화\n","  df = df.iloc[:,:] \n","  # 기존 row name을 data에 맞게 변경\n","  df = df[['videoId','comment']]\n","  df = df.rename(columns={\"videoId\": \"id\", \"comment\": \"sentence\"})\n","  df\n","\n","  # 잘 되었는지 확인\n","  tqdm.pandas()\n","  df['comment_cut'] = df['sentence'].apply(lambda x : text_preprocess(str(x))) # srt(x) 안해주면 오류 나는 경우 있음.... 왜?\n","  #df['comment_cut'] = df['sentence'].apply(lambda x : text_preprocess(x))\n","  df['comment_cut'] = df['comment_cut'].progress_apply(lambda x: tokenize(x))\n","  df\n","\n","  # 점수 내기\n","  test = Aurora3(df,sent_dic)\n","  res = test.get_df()\n","\n","  # score 결과 원하는 디렉토리에 파일로 저장\n","  score_res = pd.DataFrame(res)\n","  score_res.to_csv(f\"../data_score/score_{f_names_new[i]}.csv\", header=True, index=True)"],"execution_count":null,"outputs":[]}]}